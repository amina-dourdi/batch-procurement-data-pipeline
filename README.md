# batch-procurement-data-pipeline
Simplified batch-oriented data pipeline for a procurement system using HDFS, Presto, Hive Metastore, and PostgreSQL. The project demonstrates core data engineering concepts such as data ingestion, metadata management, batch processing, net demand calculation, and supplier order generation.


# üöÄ Procurement Data Pipeline

A **production-style batch data pipeline** that simulates, processes, validates, and transforms procurement data using **Python, HDFS, and Trino**.

This project demonstrates how modern data engineering pipelines work end-to-end:
from **raw ingestion** ‚Üí **SQL processing** ‚Üí **analytics outputs** ‚Üí **data governance**.

---

## üß† What this project shows

* How to build a **real data lake pipeline**
* How to use **Trino** as a distributed SQL engine
* How to store data in **Avro ‚Üí Parquet**
* How to implement **data quality & exception handling**
* How to make pipelines **re-runnable by date**

---

## üèóÔ∏è Architecture
###  Docker architecture
![alt text](image.png)
###  Pipeline architecture
![alt text](image-1.png)


#### Main steps

- Environment setup

- HDFS folder creation

- Data generation (faker / simulation)

- Aggregation of orders (Trino)

- Net demand computation (Trino)

- Supplier order generation (Trino)

- Data quality checks

- Exception report export

---

## üìÅ Repository Structure

```
.
‚îú‚îÄ‚îÄscripts/
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator_scheduler.py          # Main pipeline runner
‚îÇ   ‚îú‚îÄ‚îÄ generate_daily_files.py # Raw data simulation (Avro)
‚îÇ   ‚îú‚îÄ‚îÄ aggregate_orders.py     # Sales aggregation (Trino)
‚îÇ   ‚îú‚îÄ‚îÄ net_demand.py           # Net demand calculation
‚îÇ   ‚îú‚îÄ‚îÄ supplier_orders.py     # Purchase order generation
‚îÇ   ‚îú‚îÄ‚îÄ data_quality.py        # DataQualityGuard
‚îÇ   ‚îú‚îÄ‚îÄ pg_client.py
‚îÇ   ‚îî‚îÄ‚îÄ hdfs_client.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îú‚îÄ‚îÄ output/
‚îÇ   ‚îî‚îÄ‚îÄ logs/
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

---

## üîÑ Data Flow

### 1Ô∏è‚É£ Raw layer (Avro ‚Äì HDFS)

```
/raw/orders/{RUN_DATE}/
/raw/stock/{RUN_DATE}/
```

### 2Ô∏è‚É£ Processed layer (Parquet ‚Äì HDFS)

```
/processed/aggregated_orders/{RUN_DATE}/
/processed/net_demand/{RUN_DATE}/
```

### 3Ô∏è‚É£ Output layer

```
/output/supplier_orders/{RUN_DATE}/
```

### 4Ô∏è‚É£ Governance

```
/logs/exceptions/date={RUN_DATE}/exceptions.csv
```

---

## ‚öôÔ∏è Tech Stack

* **Python 3.10+**
* **Trino** ‚Äì distributed SQL engine
* **HDFS** ‚Äì data lake storage
* **PostgreSQL** ‚Äì reference & control data
* **Docker / Docker-Compose**
* **Avro** ‚Äì raw ingestion format
* **Parquet** ‚Äì analytics format

---

## üöÄ Pipeline Execution

### Run for today

```bash
python orchestrator.py
```

### Run for a specific date

```bash
RUN_DATE=2025-12-20 python orchestrator.py
```

### Run with Docker

```bash
docker-compose up --build
```

---

## üîé What happens in the pipeline

### Step 0 ‚Äî Setup

* Create HDFS folder structure
* Load environment variables
* Initialize **DataQualityGuard**

### Step 1 ‚Äî Data Generation

* Simulates daily orders & stock
* Writes **Avro** locally
* Uploads to HDFS

### Step 2 ‚Äî Aggregation (Trino)

* Reads Avro from HDFS
* Aggregates orders per SKU & market
* Writes results in **Parquet**

### Step 3 ‚Äî Net Demand

* Joins sales with stock
* Computes net demand

### Step 4 ‚Äî Supplier Orders

* Applies MOQ & packaging rules
* Generates purchase quantities

### Step 5 ‚Äî Data Quality

* Missing market files
* Unknown SKUs
* Stock inconsistencies

### Step 6 ‚Äî Reporting

* Generates **exceptions report**
* Uploads to HDFS

---

## üõ°Ô∏è Data Quality Rules

| Rule            | Description              | Severity |
| --------------- | ------------------------ | -------- |
| MISSING_FILE    | Market did not send data | MEDIUM   |
| UNKNOWN_PRODUCT | SKU not in reference     | HIGH     |
| STOCK_LOGIC     | Reserved > Available     | HIGH     |
| PIPELINE_CRASH  | System failure           | CRITICAL |

All issues are saved in:

```
/logs/exceptions/date={RUN_DATE}/exceptions.csv
```

---

## üåç Environment Variables

| Variable      | Purpose         | Example                                      |
| ------------- | --------------- | -------------------------------------------- |
| RUN_DATE      | Processing date | 2025-12-20                                   |
| HDFS_BASE_URL | HDFS namenode   | [http://namenode:9870](http://namenode:9870) |
| HDFS_USER     | HDFS user       | root                                         |
| TRINO_HOST    | Trino service   | trino                                        |
| TRINO_PORT    | Trino port      | 8080                                         |

---

## üéØ Why this project matters

This is not a simple script ‚Äî it demonstrates:

* **Modern data lake architecture**
* **Schema-on-read processing**
* **Separation of storage & compute**
* **Reproducible batch pipelines**
* **Production-style logging**
* **Data governance mindset**

It is a strong **portfolio project** for:

* Data Engineering
* Big Data Systems
* Cloud Pipelines
* Analytics Engineering

---

## üîÆ Future Improvements

* Add **Airflow** for scheduling
* Add **Hive Metastore** for metadata
* Add **Superset** dashboards
* Add **Kafka** for streaming ingestion
* Add **Great Expectations** for data quality

